const fs = require('fs').promises;
const path = require('path');
const archiver = require('archiver');
const { createWriteStream, createReadStream } = require('fs');
const redis = require('../models/redis');
const logger = require('../utils/logger');

class BackupService {
  constructor() {
    // é»˜è®¤å¤‡ä»½è·¯å¾„ - ä¸init.jsonåŒçº§
    this.defaultBackupPath = path.join(process.cwd(), 'data', 'backups');
    // ä¸´æ—¶æ–‡ä»¶è·¯å¾„
    this.tempPath = path.join(process.cwd(), 'temp');
    // ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
    this.ensureDirectories();
  }

  async ensureDirectories() {
    try {
      const backupPath = await this.getBackupPath();
      await fs.mkdir(backupPath, { recursive: true });
      await fs.mkdir(this.tempPath, { recursive: true });
    } catch (error) {
      logger.error('âŒ Failed to create directories:', error);
    }
  }

  // è·å–å¤‡ä»½è·¯å¾„ï¼ˆä»Redisæˆ–ä½¿ç”¨é»˜è®¤å€¼ï¼‰
  async getBackupPath() {
    try {
      const client = redis.getClientSafe();
      const settings = await client.get('backup:settings');
      if (settings) {
        const parsedSettings = JSON.parse(settings);
        return parsedSettings.backupPath || this.defaultBackupPath;
      }
    } catch (error) {
      logger.warn('âš ï¸ Failed to get backup path from Redis, using default:', error.message);
    }
    return this.defaultBackupPath;
  }

  // è·å–å¤‡ä»½è®¾ç½®
  async getBackupSettings() {
    try {
      const client = redis.getClientSafe();
      const settings = await client.get('backup:settings');
      if (settings) {
        return JSON.parse(settings);
      }
    } catch (error) {
      logger.warn('âš ï¸ Failed to get backup settings:', error.message);
    }
    
    // è¿”å›é»˜è®¤è®¾ç½®
    return {
      autoBackupEnabled: false,
      autoBackupInterval: 24, // é»˜è®¤24å°æ—¶
      backupPath: this.defaultBackupPath,
      maxBackups: 10 // æœ€å¤šä¿ç•™10ä¸ªå¤‡ä»½
    };
  }

  // æ›´æ–°å¤‡ä»½è®¾ç½®
  async updateBackupSettings(settings) {
    try {
      const client = redis.getClientSafe();
      
      // éªŒè¯å¤‡ä»½è·¯å¾„
      if (settings.backupPath) {
        await fs.mkdir(settings.backupPath, { recursive: true });
        // æµ‹è¯•å†™å…¥æƒé™
        const testFile = path.join(settings.backupPath, '.test');
        await fs.writeFile(testFile, 'test');
        await fs.unlink(testFile);
      }
      
      await client.set('backup:settings', JSON.stringify(settings));
      logger.info('âœ… Backup settings updated successfully');
      return true;
    } catch (error) {
      logger.error('âŒ Failed to update backup settings:', error);
      throw error;
    }
  }

  // é‡ç½®å¤‡ä»½è®¾ç½®ä¸ºé»˜è®¤å€¼
  async resetBackupSettings() {
    try {
      const client = redis.getClientSafe();
      await client.del('backup:settings');
      logger.info('âœ… Backup settings reset to defaults');
      return this.getBackupSettings(); // è¿”å›é»˜è®¤è®¾ç½®
    } catch (error) {
      logger.error('âŒ Failed to reset backup settings:', error);
      throw error;
    }
  }

  // åˆ›å»ºå¤‡ä»½ï¼ˆJSONæ ¼å¼ï¼‰
  async createBackup() {
    const startTime = Date.now();
    const backupId = `backup_${new Date().toISOString().replace(/[:.]/g, '-')}`;
    const tempDir = path.join(this.tempPath, backupId);
    
    try {
      logger.info(`ğŸ“¦ Starting backup: ${backupId}`);
      
      const client = redis.getClientSafe();
      const backupPath = await this.getBackupPath();
      
      // åˆ›å»ºä¸´æ—¶ç›®å½•
      await fs.mkdir(tempDir, { recursive: true });
      
      // è·å–æ‰€æœ‰é”®
      const keys = await client.keys('*');
      logger.info(`ğŸ“Š Found ${keys.length} keys to backup`);
      
      // å¯¼å‡ºæ•°æ®åˆ°JSONæ–‡ä»¶
      const data = {
        keys: []
      };
      
      let processedCount = 0;
      const batchSize = 100;
      
      for (let i = 0; i < keys.length; i += batchSize) {
        const batch = keys.slice(i, i + batchSize);
        
        for (const key of batch) {
          try {
            const type = await client.type(key);
            const ttl = await client.ttl(key);
            
            let value;
            switch (type) {
              case 'string':
                value = await client.get(key);
                break;
              case 'hash':
                value = await client.hgetall(key);
                break;
              case 'list':
                value = await client.lrange(key, 0, -1);
                break;
              case 'set':
                value = await client.smembers(key);
                break;
              case 'zset': {
                // å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ Redis å®¢æˆ·ç«¯
                let members;
                try {
                  // æ–°ç‰ˆæœ¬è¯­æ³•
                  members = await client.zrange(key, 0, -1, { WITHSCORES: true });
                } catch (err) {
                  // æ—§ç‰ˆæœ¬è¯­æ³•
                  members = await client.zrange(key, 0, -1, 'WITHSCORES');
                }
                value = [];
                for (let j = 0; j < members.length; j += 2) {
                  value.push({ member: members[j], score: parseFloat(members[j + 1]) });
                }
                break;
              }
              default:
                logger.warn(`âš ï¸ Unsupported type ${type} for key ${key}`);
                continue;
            }
            
            data.keys.push({
              key,
              type,
              value,
              ttl: ttl > 0 ? ttl : -1
            });
            
            processedCount++;
          } catch (error) {
            logger.error(`âŒ Failed to backup key ${key}:`, error.message);
          }
        }
        
        // è¿›åº¦æç¤º
        if (processedCount % 100 === 0) {
          logger.info(`ğŸ“Š Progress: ${processedCount}/${keys.length} keys processed`);
        }
      }
      
      // ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶
      const dataPath = path.join(tempDir, 'data.json');
      await fs.writeFile(dataPath, JSON.stringify(data, null, 2));
      
      // åˆ›å»ºå…ƒæ•°æ®
      const metadata = {
        backupId,
        timestamp: new Date().toISOString(),
        version: '3.0',
        format: 'json',
        keysCount: data.keys.length,
        serviceName: 'Claude Relay Service',
        files: {
          'data.json': 'All Redis data in JSON format',
          'metadata.json': 'Backup metadata'
        }
      };
      
      // ä¿å­˜å…ƒæ•°æ®
      await fs.writeFile(
        path.join(tempDir, 'metadata.json'),
        JSON.stringify(metadata, null, 2)
      );
      
      // åˆ›å»ºå‹ç¼©åŒ…
      const zipPath = path.join(backupPath, `${backupId}.zip`);
      await this.createZipArchive(tempDir, zipPath);
      
      // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
      await fs.rm(tempDir, { recursive: true, force: true });
      
      // è·å–æ–‡ä»¶ä¿¡æ¯
      const stat = await fs.stat(zipPath);
      const backupInfo = {
        id: backupId,
        fileName: `${backupId}.zip`,
        filePath: zipPath,
        timestamp: new Date().toISOString(),
        size: stat.size,
        keysCount: data.keys.length,
        duration: Date.now() - startTime,
        format: 'zip',
        version: '3.0'
      };
      
      logger.success(`âœ… Backup completed: ${backupId} (${backupInfo.duration}ms)`);
      
      // æ¸…ç†æ—§å¤‡ä»½
      await this.cleanupOldBackups();
      
      return backupInfo;
    } catch (error) {
      // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
      try {
        await fs.rm(tempDir, { recursive: true, force: true });
      } catch (cleanupError) {
        logger.warn('âš ï¸ Failed to cleanup temp files:', cleanupError.message);
      }
      
      logger.error(`âŒ Backup failed: ${backupId}`, error);
      throw error;
    }
  }

  // åˆ›å»º ZIP å‹ç¼©åŒ…
  async createZipArchive(sourceDir, destPath) {
    return new Promise((resolve, reject) => {
      const output = createWriteStream(destPath);
      const archive = archiver('zip', {
        zlib: { level: 9 } // æœ€é«˜å‹ç¼©çº§åˆ«
      });

      output.on('close', () => {
        logger.info(`ğŸ“¦ Archive created: ${archive.pointer()} bytes`);
        resolve();
      });

      archive.on('error', (err) => {
        reject(err);
      });

      archive.pipe(output);
      archive.directory(sourceDir, false);
      archive.finalize();
    });
  }

  // è·å–å¤‡ä»½å†å²ï¼ˆæ‰«æç›®å½•ï¼‰
  async getBackupHistory(limit = 20) {
    try {
      const backupPath = await this.getBackupPath();
      const backups = [];
      
      // ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
      await fs.mkdir(backupPath, { recursive: true });
      
      // è¯»å–ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
      const files = await fs.readdir(backupPath);
      
      // è¿‡æ»¤å‡ºå¤‡ä»½æ–‡ä»¶
      const backupFiles = files.filter(file => file.startsWith('backup_') && file.endsWith('.zip'));
      
      // è·å–æ¯ä¸ªå¤‡ä»½æ–‡ä»¶çš„ä¿¡æ¯
      for (const fileName of backupFiles) {
        try {
          const filePath = path.join(backupPath, fileName);
          const stat = await fs.stat(filePath);
          
          // ä»æ–‡ä»¶åæå–å¤‡ä»½IDå’Œæ—¶é—´æˆ³
          const id = fileName.replace('.zip', '');
          // backup_2025-07-31T08-13-18-066Z -> 2025-07-31T08:13:18.066Z
          const timestampMatch = id.match(/backup_(\d{4}-\d{2}-\d{2}T\d{2}-\d{2}-\d{2}-\d{3}Z)/);
          if (!timestampMatch) continue;
          
          const timestamp = timestampMatch[1].replace(/-(\d{2})-(\d{2})-(\d{3}Z)$/, ':$1:$2.$3');
          
          const backupInfo = {
            id: id,
            fileName: fileName,
            filePath: filePath,
            timestamp: timestamp,
            size: stat.size,
            keysCount: 0, // æ— æ³•ä»æ–‡ä»¶åè·å–ï¼Œè®¾ä¸º0
            exists: true,
            format: 'zip',
            version: '3.0'
          };
          
          backups.push(backupInfo);
        } catch (error) {
          logger.warn(`âš ï¸ Failed to read backup file ${fileName}:`, error.message);
        }
      }
      
      // æŒ‰æ—¶é—´æˆ³é™åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
      backups.sort((a, b) => new Date(b.timestamp) - new Date(a.timestamp));
      
      // é™åˆ¶è¿”å›æ•°é‡
      return backups.slice(0, limit);
    } catch (error) {
      logger.error('âŒ Failed to get backup history:', error);
      return [];
    }
  }

  // è¿˜åŸå¤‡ä»½
  async restoreBackup(backupId) {
    const startTime = Date.now();
    const tempDir = path.join(this.tempPath, `restore_${backupId}`);
    
    try {
      logger.info(`ğŸ”„ Starting restore: ${backupId}`);
      
      const backupPath = await this.getBackupPath();
      const zipPath = path.join(backupPath, `${backupId}.zip`);
      
      // æ£€æŸ¥å¤‡ä»½æ–‡ä»¶æ˜¯å¦å­˜åœ¨
      await fs.access(zipPath);
      
      // åˆ›å»ºä¸´æ—¶ç›®å½•
      await fs.mkdir(tempDir, { recursive: true });
      
      // è§£å‹å¤‡ä»½æ–‡ä»¶
      await this.extractZipArchive(zipPath, tempDir);
      
      // è¯»å–å…ƒæ•°æ®
      const metadataPath = path.join(tempDir, 'metadata.json');
      const metadataContent = await fs.readFile(metadataPath, 'utf8');
      const metadata = JSON.parse(metadataContent);
      
      logger.info(`ğŸ“Š Restoring backup version ${metadata.version}, format: ${metadata.format}`);
      
      // æ ¹æ®ç‰ˆæœ¬é€‰æ‹©è¿˜åŸæ–¹æ³•
      let restoredCount;
      if (metadata.format === 'json' && metadata.version === '3.0') {
        restoredCount = await this.restoreFromJSON(tempDir);
      } else if (metadata.format === 'rdb') {
        // å¯¹äºRDBæ ¼å¼ï¼Œæˆ‘ä»¬éœ€è¦è½¬æ¢ä¸ºJSONæ ¼å¼å†è¿˜åŸ
        throw new Error('RDB format restore requires manual Redis restart. Please use JSON format backups.');
      } else {
        throw new Error(`Unsupported backup format: ${metadata.format} v${metadata.version}`);
      }
      
      // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
      await fs.rm(tempDir, { recursive: true, force: true });
      
      const duration = Date.now() - startTime;
      logger.success(`âœ… Restore completed: ${backupId} (${duration}ms)`);
      
      return {
        backupId,
        restoredKeys: restoredCount,
        duration
      };
    } catch (error) {
      // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
      try {
        await fs.rm(tempDir, { recursive: true, force: true });
      } catch (cleanupError) {
        logger.warn('âš ï¸ Failed to cleanup temp files:', cleanupError.message);
      }
      
      logger.error(`âŒ Restore failed: ${backupId}`, error);
      throw error;
    }
  }

  // ä»JSONè¿˜åŸæ•°æ®
  async restoreFromJSON(tempDir) {
    const client = redis.getClientSafe();
    
    try {
      // è¯»å–æ•°æ®æ–‡ä»¶
      const dataPath = path.join(tempDir, 'data.json');
      const dataContent = await fs.readFile(dataPath, 'utf8');
      const data = JSON.parse(dataContent);
      
      if (!data.keys || !Array.isArray(data.keys)) {
        throw new Error('Invalid backup data format');
      }
      
      logger.info(`ğŸ“Š Found ${data.keys.length} keys to restore`);
      
      // ä¿å­˜é‡è¦æ•°æ®
      logger.info('ğŸ“¦ Saving important data before restore...');
      const backupSettings = await client.get('backup:settings');
      
      // æ¸…ç©ºæ•°æ®åº“
      logger.info('ğŸ§¹ Clearing current database...');
      await client.flushdb();
      
      // è¿˜åŸæ•°æ®
      let restoredCount = 0;
      const batchSize = 100;
      
      for (let i = 0; i < data.keys.length; i += batchSize) {
        const batch = data.keys.slice(i, i + batchSize);
        
        for (const item of batch) {
          try {
            const { key, type, value, ttl } = item;
            
            switch (type) {
              case 'string':
                await client.set(key, value);
                break;
              case 'hash':
                if (Object.keys(value).length > 0) {
                  await client.hset(key, value);
                }
                break;
              case 'list':
                if (value.length > 0) {
                  await client.rpush(key, ...value);
                }
                break;
              case 'set':
                if (value.length > 0) {
                  await client.sadd(key, ...value);
                }
                break;
              case 'zset':
                if (value.length > 0) {
                  const zsetData = [];
                  for (const item of value) {
                    zsetData.push(item.score, item.member);
                  }
                  await client.zadd(key, ...zsetData);
                }
                break;
              default:
                logger.warn(`âš ï¸ Unsupported type ${type} for key ${key}`);
                continue;
            }
            
            // è®¾ç½®TTL
            if (ttl > 0) {
              await client.expire(key, ttl);
            }
            
            restoredCount++;
          } catch (error) {
            logger.error(`âŒ Failed to restore key ${item.key}:`, error.message);
          }
        }
        
        // è¿›åº¦æç¤º
        if (restoredCount % 100 === 0) {
          logger.info(`ğŸ”„ Progress: ${restoredCount}/${data.keys.length} keys restored`);
        }
      }
      
      // æ¢å¤é‡è¦æ•°æ®
      logger.info('ğŸ“‹ Restoring backup settings...');
      if (backupSettings) {
        await client.set('backup:settings', backupSettings);
      }
      
      // éªŒè¯æ•°æ®
      const keys = await client.keys('*');
      logger.info(`ğŸ“Š Restored data stats: ${keys.length} keys`);
      
      // ç»Ÿè®¡ä¸åŒç±»å‹çš„é”®
      const apiKeys = keys.filter(k => k.startsWith('api_key:')).length;
      const apiKeyHashes = keys.filter(k => k.startsWith('api_key_hash:')).length;
      const claudeAccounts = keys.filter(k => k.startsWith('claude_account:')).length;
      const geminiAccounts = keys.filter(k => k.startsWith('gemini_account:')).length;
      const usage = keys.filter(k => k.startsWith('usage:')).length;
      const admins = keys.filter(k => k.startsWith('admin:')).length;
      
      logger.info(`  - API Keys: ${apiKeys}`);
      logger.info(`  - API Key Hashes: ${apiKeyHashes}`);
      logger.info(`  - Claude Accounts: ${claudeAccounts}`);
      logger.info(`  - Gemini Accounts: ${geminiAccounts}`);
      logger.info(`  - Admins: ${admins}`);
      logger.info(`  - Usage Records: ${usage}`);
      
      logger.success(`âœ… Restored ${restoredCount} keys successfully!`);
      
      return restoredCount;
    } catch (error) {
      logger.error('âŒ JSON restore failed:', error);
      throw error;
    }
  }

  // è§£å‹ ZIP æ–‡ä»¶
  async extractZipArchive(zipPath, destDir) {
    const unzipper = require('unzipper');
    
    return new Promise((resolve, reject) => {
      createReadStream(zipPath)
        .pipe(unzipper.Extract({ path: destDir }))
        .on('close', resolve)
        .on('error', reject);
    });
  }

  // åˆ é™¤å¤‡ä»½
  async deleteBackup(backupId) {
    try {
      const backupPath = await this.getBackupPath();
      const fileName = `${backupId}.zip`;
      const filePath = path.join(backupPath, fileName);
      
      // åˆ é™¤zipæ–‡ä»¶
      try {
        await fs.access(filePath);
        await fs.unlink(filePath);
        logger.info(`ğŸ—‘ï¸ Deleted backup: ${fileName}`);
      } catch (error) {
        logger.warn(`âš ï¸ Backup file not found: ${fileName}`);
        // æ–‡ä»¶ä¸å­˜åœ¨æ—¶ä¸æŠ›å‡ºé”™è¯¯ï¼Œåªè®°å½•è­¦å‘Š
        return false;
      }
      
      return true;
    } catch (error) {
      logger.error(`âŒ Failed to delete backup ${backupId}:`, error);
      throw error;
    }
  }

  // æ¸…ç†æ—§å¤‡ä»½
  async cleanupOldBackups() {
    try {
      const settings = await this.getBackupSettings();
      const maxBackups = settings.maxBackups || 10;
      
      const history = await this.getBackupHistory(1000);
      const validBackups = history.filter(b => b.exists);
      
      if (validBackups.length > maxBackups) {
        const toDelete = validBackups.slice(maxBackups);
        for (const backup of toDelete) {
          try {
            await this.deleteBackup(backup.id);
          } catch (error) {
            logger.warn(`âš ï¸ Failed to cleanup old backup ${backup.id}:`, error.message);
          }
        }
      }
    } catch (error) {
      logger.error('âŒ Failed to cleanup old backups:', error);
    }
  }

  // è·å–å¤‡ä»½æ–‡ä»¶è·¯å¾„
  async getBackupFilePath(backupId) {
    const backupPath = await this.getBackupPath();
    const fileName = `${backupId}.zip`;
    const filePath = path.join(backupPath, fileName);
    
    // æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    await fs.access(filePath);
    return filePath;
  }

  // å¯¼å…¥å¤–éƒ¨å¤‡ä»½æ–‡ä»¶
  async importBackup(filePath, options = {}) {
    const tempDir = path.join(this.tempPath, `import_${Date.now()}`);
    
    try {
      logger.info(`ğŸ“¥ Importing backup from: ${filePath}`);
      
      // åˆ›å»ºä¸´æ—¶ç›®å½•
      await fs.mkdir(tempDir, { recursive: true });
      
      // è§£å‹å¤‡ä»½æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
      await this.extractZipArchive(filePath, tempDir);
      
      // è¯»å–å¹¶éªŒè¯å…ƒæ•°æ®
      const metadataPath = path.join(tempDir, 'metadata.json');
      const metadataContent = await fs.readFile(metadataPath, 'utf8');
      const metadata = JSON.parse(metadataContent);
      
      // éªŒè¯å¤‡ä»½æ ¼å¼
      if (!metadata.format || !metadata.version) {
        throw new Error('Invalid backup format: missing metadata');
      }
      
      // ç”Ÿæˆæ–°çš„å¤‡ä»½IDï¼ˆä½¿ç”¨åŸå§‹æ—¶é—´æˆ³ï¼‰
      const importedBackupId = `backup_${new Date(metadata.timestamp).toISOString().replace(/[:.]/g, '-')}`;
      
      // å¤åˆ¶åˆ°å¤‡ä»½ç›®å½•
      const backupPath = await this.getBackupPath();
      const destPath = path.join(backupPath, `${importedBackupId}.zip`);
      await fs.copyFile(filePath, destPath);
      
      // åˆ›å»ºå¤‡ä»½ä¿¡æ¯ï¼ˆä»…ç”¨äºè¿”å›ï¼‰
      const stat = await fs.stat(destPath);
      const backupInfo = {
        id: importedBackupId,
        fileName: `${importedBackupId}.zip`,
        filePath: destPath,
        timestamp: metadata.timestamp,
        size: stat.size,
        keysCount: metadata.keysCount,
        duration: 0,
        format: 'zip',
        version: metadata.version,
        imported: true,
        importedAt: new Date().toISOString()
      };
      
      // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
      await fs.rm(tempDir, { recursive: true, force: true });
      
      logger.success(`âœ… Backup imported successfully: ${importedBackupId}`);
      
      // å¦‚æœé€‰é¡¹ä¸­æŒ‡å®šäº†ç«‹å³è¿˜åŸ
      if (options.restore) {
        logger.info('ğŸ”„ Restoring imported backup...');
        await this.restoreBackup(importedBackupId);
      }
      
      return {
        ...backupInfo,
        metadata
      };
    } catch (error) {
      // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
      try {
        await fs.rm(tempDir, { recursive: true, force: true });
      } catch (cleanupError) {
        logger.warn('âš ï¸ Failed to cleanup temp files:', cleanupError.message);
      }
      
      logger.error('âŒ Failed to import backup:', error);
      throw error;
    }
  }

  // éªŒè¯å¤‡ä»½æ–‡ä»¶ï¼ˆä¸å¯¼å…¥ï¼‰
  async validateBackupFile(filePath) {
    const tempDir = path.join(this.tempPath, `validate_${Date.now()}`);
    
    try {
      // åˆ›å»ºä¸´æ—¶ç›®å½•
      await fs.mkdir(tempDir, { recursive: true });
      
      // è§£å‹å¤‡ä»½æ–‡ä»¶
      await this.extractZipArchive(filePath, tempDir);
      
      // è¯»å–å…ƒæ•°æ®
      const metadataPath = path.join(tempDir, 'metadata.json');
      const metadataContent = await fs.readFile(metadataPath, 'utf8');
      const metadata = JSON.parse(metadataContent);
      
      // éªŒè¯å¿…è¦æ–‡ä»¶
      if (metadata.format === 'json') {
        const dataPath = path.join(tempDir, 'data.json');
        await fs.access(dataPath);
      } else if (metadata.format === 'rdb') {
        const rdbPath = path.join(tempDir, 'redis.rdb');
        await fs.access(rdbPath);
      }
      
      // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
      await fs.rm(tempDir, { recursive: true, force: true });
      
      return {
        valid: true,
        metadata
      };
    } catch (error) {
      // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
      try {
        await fs.rm(tempDir, { recursive: true, force: true });
      } catch (cleanupError) {
        // å¿½ç•¥æ¸…ç†é”™è¯¯
      }
      
      return {
        valid: false,
        error: error.message
      };
    }
  }
}

module.exports = new BackupService();